# ğŸ› ï¸ Linux Device Driver Coding Model Evaluation System

This project implements a Python-based evaluation framework to benchmark the quality of Linux device driver code generated by AI coding models (e.g., ChatGPT, Code Llama, etc.). It scores the output across various metrics like compilation success, kernel best practices, security, and style compliance.

---

## ğŸ“Œ Objective

To automate the evaluation of Linux kernel driver code generated by AI models and assign meaningful scores based on:

- âœ… Compilation success
- ğŸ›¡ï¸ Security and safety
- ğŸ§  Kernel programming best practices
- ğŸ§¹ Code quality and maintainability
- âš™ï¸ Performance-related heuristics

---


---

## ğŸ§ª How It Works

1. An AI model is given prompts to generate `.c` Linux device driver code.
2. The generated file is placed inside the `drivers/` folder.
3. The evaluation system:
   - Reads the C code
   - Runs a static analysis pass
   - Tries compiling it with GCC
   - Assigns scores based on predefined rules
4. Scores are saved to `results/evaluation_output.json`.

---

## ğŸ§® Example Evaluation Output

```json
{
  "compilation": {
    "success_rate": 1.0,
    "warnings_count": 0,
    "errors_count": 0
  },
  "security": {
    "buffer_safety": 1,
    "race_conditions": 1,
    "input_validation": 1
  },
  "functionality": {
    "basic_operations": 0.9,
    "error_handling": 0.7,
    "edge_cases": 0.6
  },
  "code_quality": {
    "style_compliance": 0.9,
    "documentation": 0.6,
    "maintainability": 0.7
  },
  "overall_score": 86.5
}
